#    https://keras.io/getting-started/sequential-model-guide/#examples
#  check this out
#  check this out for shaping input and outputs#  check this out for shaping input and outputs#  check this out for shaping input and outputs#  check this out for shaping input and outputs
#  check this out for shaping input and outputs#  check this out for shaping input and outputs#  check this out for shaping input and outputs#  check this out for shaping input and outputs#  check this out for shaping input and outputs
#  check this out for shaping input and outputs#  check this out for shaping input and outputs#  check this out for shaping input and outputs#  check this out for shaping input and outputs




from keras.models import Sequential
from keras.layers import LSTM, Dense, TimeDistributed
from keras.utils import to_categorical
import numpy as np

# x_train = np.zeros((1000, None, 2))
# x_train[1, ] = np.array([[0,   1], [0.1,   0.9], [0.2, 0.8], [0.3, 0.7]]) 


import numpy as np
from keras.models import load_model
model = load_model('./me_ep1000_var61_acc90.h5') #./math_equation.h5', custom_objects = { '0':'a', '1':'b', '2': 'c', '3':'d'})
print(model)



# #the batch size of 1 test sample
# # x_test_up = np.array([[0,     0.1], [0.1,     0.3], [0.2,     0.5], [0.3, 0.7], [0.4,     0.9], [0.5,     1.1], 
# # 	[0.6,     1.3], [0.7, 1.5], [0.8,     1.7], [0.9,     1.9]]) 
# x_test_down_short = np.array([[0,   1], [0.1,   0.9], [0.2, 0.8], [0.3, 0.7], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]])  
# x_test_up_down  = np.array([[0,     0.1], [0.1,     0.3], [0.2,     0.5], [0.3, 0.7], [0.4,     0.9], [0.5,     .6], [0.6,     .5], [0.7, .4], [0.8,     .3], [0.9,     .2]]) 
# x_test_up_flat = np.array([[0,     0.1], [0.1,     0.3], [0.2,     0.5], [0.3, 0.7], [0.4,     0.9], [0.5,     0.9], [0.6,     0.9], [0.7, 0.9], [0.8,     0.9], [0.9,     0.9]]) 
# x_test_down_up  = np.array([[0,   1], [0.1,   0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6], [0.5, .6], [0.6,  .7], [0.7,  .8], [0.8,  .9], [0.9,  1]]) 
# x_test_decreasing  = np.array([[0,   1], [0.1,   0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6], [0.5, .5], [0.6,  .4], [0.7,  .3], [0.8,  .2], [0.9,  .1]]) 
# x_test_up_short  = np.array([[0,     0.1], [0.1,     0.3], [0.2,     0.5], [0.3, 0.7], [0.4,     0.9], [0.0, 0.0],  [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]) 

# x_test = np.stack((x_test_down_short, x_test_up_down, x_test_up_flat, x_test_down_up, x_test_decreasing, x_test_up_short))#, axis=1)
# y_test = np.array( [2,0,0,1,1, 2] )
# #print(x_test)
# #print(x_test.shape)


# #x_test_up = x_test_up.reshape(1,4,2)
# #print( model.predict(x_test_up, batch_size=None, verbose=1) )
# scores = model.evaluate(x_test, to_categorical(y_test, 4), verbose=1)
# print(scores)
# print('training data results: ')
# for i in range(len(model.metrics_names)):
#     print(str(model.metrics_names[i]) + ": " +  str(scores[i]*100))

y_1_1 = [[  4.167 , 11.224],
 [  4.167 , 12.245],
 [  0.    ,  7.143],
 [  0.    ,  5.102],
 [ 25.    ,  0.   ],
 [ 29.167 ,  0.   ],
 [ 31.25  ,  0.   ],
 [ 35.417 ,  1.02 ],
 [ 37.5   ,  1.02 ],
 [ 39.583 ,  2.041],
 [ 41.667 ,  3.061],
 [ 43.75  ,  5.102],
 [ 43.75  ,  7.143],
 [ 43.75  ,  8.163],
 [ 43.75  , 10.204],
 [ 43.75  , 12.245],
 [ 43.75  , 13.265],
 [ 43.75  , 15.306],
 [ 45.833 , 16.327],
 [ 45.833 , 18.367],
 [ 47.917 , 19.388],
 [ 50.    , 20.408],
 [ 52.083 , 21.429],
 [ 56.25  , 22.449],
 [ 58.333 , 22.449],
 [ 62.5   , 22.449],
 [ 66.667 , 22.449],
 [ 70.833 , 22.449],
 [ 75.    , 21.429],
 [ 81.25  , 20.408],
 [ 85.417 , 19.388],
 [ 89.583 , 17.347],
 [ 91.667 , 16.327],
 [ 93.75  , 15.306],
 [ 95.833 , 13.265],
 [ 97.917 , 11.224],
 [100.    , 10.204],
 [100.    ,  9.184],
 [100.    ,  7.143],
 [100.    ,  6.122],
 [100.    ,  5.102],
 [100.    ,  4.082],
 [100.    ,  3.061],
 [100.    ,  2.041],
 [ 97.917 ,  2.041],
 [ 97.917 ,  3.061],
 [ 97.917 ,  4.082],
 [ 95.833 ,  5.102],
 [ 93.75  ,  6.122],
 [ 93.75  ,  8.163],
 [ 91.667 ,  9.184],
 [ 89.583 , 11.224],
 [ 87.5   , 14.286],
 [ 87.5   , 16.327],
 [ 85.417 , 18.367],
 [ 83.333 , 21.429],
 [ 83.333 , 23.469],
 [ 83.333 , 26.531],
 [ 83.333 , 29.592],
 [ 83.333 , 31.633],
 [ 83.333 , 34.694],
 [ 83.333 , 36.735],
 [ 85.417 , 39.796],
 [ 85.417 , 42.857],
 [ 85.417 , 45.918],
 [ 87.5   , 47.959],
 [ 87.5   , 51.02 ],
 [ 87.5   , 53.061],
 [ 89.583 , 56.122],
 [ 89.583 , 58.163],
 [ 89.583 , 61.224],
 [ 89.583 , 63.265],
 [ 91.667 , 65.306],
 [ 91.667 , 67.347],
 [ 91.667 , 69.388],
 [ 91.667 , 72.449],
 [ 91.667 , 74.49 ],
 [ 89.583 , 76.531],
 [ 89.583 , 78.571],
 [ 89.583 , 80.612],
 [ 87.5   , 82.653],
 [ 85.417 , 84.694],
 [ 85.417 , 85.714],
 [ 83.333 , 87.755],
 [ 81.25  , 89.796],
 [ 81.25  , 90.816],
 [ 79.167 , 91.837],
 [ 77.083 , 93.878],
 [ 75.    , 94.898],
 [ 72.917 , 95.918],
 [ 72.917 , 96.939],
 [ 70.833 , 97.959],
 [ 68.75  , 98.98 ],
 [ 66.667 , 98.98 ],
 [ 64.583 ,100.   ],
 [ 62.5   ,100.   ],
 [ 58.333 ,100.   ],
 [ 56.25  ,100.   ],
 [ 54.167 ,100.   ],
 [ 52.083 ,100.   ],
 [ 47.917 , 98.98 ],
 [ 45.833 , 98.98 ],
 [ 41.667 , 98.98 ],
 [ 39.583 , 98.98 ],
 [ 37.5   , 97.959],
 [ 33.333 , 97.959],
 [ 29.167 , 96.939],
 [ 27.083 , 95.918],
 [ 22.917 , 94.898],
 [ 20.833 , 94.898]]









































sigma_1_1 = [[ 14.856,  41.176],
 [ 12.417,  45.541],
 [  9.313,  50.474],
 [  6.652,  56.357],
 [  4.213,  62.808],
 [  2.217,  69.26 ],
 [  0.443,  75.712],
 [  0.   ,  81.594],
 [  0.222,  87.476],
 [  1.33 ,  91.841],
 [  3.326,  95.256],
 [  6.652,  97.913],
 [ 10.2  ,  99.62 ],
 [ 15.078, 100.   ],
 [ 19.956,  99.62 ],
 [ 25.055,  98.672],
 [ 30.377,  96.774],
 [ 35.477,  93.928],
 [ 40.133,  90.702],
 [ 44.346,  86.907],
 [ 47.672,  82.732],
 [ 50.554,  78.558],
 [ 52.106,  74.004],
 [ 53.437,  69.07 ],
 [ 53.437,  64.326],
 [ 52.993,  59.772],
 [ 51.663,  55.028],
 [ 49.889,  50.664],
 [ 47.228,  46.679],
 [ 44.346,  42.694],
 [ 40.798,  39.469],
 [ 37.029,  37.192],
 [ 33.038,  35.484],
 [ 29.047,  33.966],
 [ 25.499,  33.397],
 [ 21.729,  33.207],
 [ 18.404,  33.776],
 [ 15.521,  35.294],
 [ 13.304,  37.192],
 [ 11.308,  39.469],
 [ 10.421,  42.125],
 [  9.978,  44.972],
 [  9.978,  48.008],
 [ 10.643,  50.854],
 [ 12.195,  53.321],
 [ 14.191,  55.598],
 [ 16.63 ,  57.306],
 [ 19.956,  58.444],
 [ 23.947,  59.013],
 [ 28.381,  58.444],
 [ 33.259,  57.495],
 [ 38.359,  54.839],
 [ 44.568,  51.803],
 [ 50.554,  47.249],
 [ 56.763,  41.746],
 [ 63.636,  36.053],
 [ 70.067,  29.602],
 [ 76.275,  23.34 ],
 [ 83.149,  18.027],
 [ 88.692,  11.006],
 [ 95.565,   6.072],
 [100.   ,   0.   ]]

x_test = [[ 92.291,  27.71 ],
 [ 92.514,  28.087],
 [ 92.291,  27.992],
 [ 92.291,  28.087],
 [ 92.291,  28.087],
 [ 92.291,  27.992],
 [ 92.737,  28.087],
 [ 92.514,  27.804],
 [ 93.073,  27.804],
 [ 93.073,  27.333],
 [ 93.52 ,  27.05 ],
 [ 93.631,  26.39 ],
 [ 93.966,  25.636],
 [ 94.19 ,  24.599],
 [ 94.637,  23.563],
 [ 94.86 ,  22.149],
 [ 95.419,  20.547],
 [ 95.866,  18.567],
 [ 96.76 ,  16.494],
 [ 97.207,  14.326],
 [ 97.877,  11.97 ],
 [ 98.547,   9.708],
 [ 99.218,   7.634],
 [ 99.777,   5.561],
 [100.   ,   3.77 ],
 [100.   ,   2.262],
 [ 99.888,   1.32 ],
 [ 99.218,   0.471],
 [ 98.324,   0.189],
 [ 96.983,   0.   ],
 [ 95.084,   0.   ],
 [ 92.737,   0.283],
 [ 89.944,   0.566],
 [ 86.369,   0.848],
 [ 82.458,   1.414],
 [ 78.101,   1.791],
 [ 73.073,   2.356],
 [ 67.486,   2.733],
 [ 61.788,   3.016],
 [ 55.754,   3.393],
 [ 49.721,   3.77 ],
 [ 44.022,   4.336],
 [ 38.659,   4.524],
 [ 33.743,   4.995],
 [ 29.385,   5.561],
 [ 25.698,   6.221],
 [ 22.905,   6.975],
 [ 20.559,   7.823],
 [ 19.106,   8.765],
 [ 18.324,   9.991],
 [ 17.877,  11.31 ],
 [ 18.324,  12.818],
 [ 19.441,  14.232],
 [ 21.229,  15.646],
 [ 23.464,  17.342],
 [ 26.369,  18.944],
 [ 30.168,  20.452],
 [ 34.302,  22.055],
 [ 38.994,  23.468],
 [ 43.575,  25.165],
 [ 48.268,  26.767],
 [ 52.737,  28.464],
 [ 56.983,  30.254],
 [ 60.559,  32.234],
 [ 63.352,  34.213],
 [ 65.475,  36.381],
 [ 66.592,  38.549],
 [ 67.039,  40.905],
 [ 66.369,  43.544],
 [ 65.251,  46.183],
 [ 63.24 ,  48.822],
 [ 60.223,  51.649],
 [ 56.425,  54.76 ],
 [ 51.955,  57.87 ],
 [ 46.927,  61.169],
 [ 41.341,  64.562],
 [ 35.531,  67.955],
 [ 29.832,  71.159],
 [ 24.022,  74.458],
 [ 18.547,  77.474],
 [ 13.743,  80.207],
 [  9.385,  82.658],
 [  6.034,  84.637],
 [  3.687,  86.428],
 [  1.564,  87.936],
 [  0.335,  89.161],
 [  0.   ,  90.292],
 [  0.335,  91.046],
 [  1.564,  91.989],
 [  3.687,  92.366],
 [  6.145,  93.025],
 [  9.162,  93.402],
 [ 13.073,  93.591],
 [ 17.542,  93.685],
 [ 22.905,  93.685],
 [ 28.38 ,  93.591],
 [ 34.413,  93.497],
 [ 40.782,  93.497],
 [ 47.374,  93.497],
 [ 54.19 ,  93.874],
 [ 60.782,  94.156],
 [ 67.151,  94.816],
 [ 72.961,  95.382],
 [ 78.212,  96.136],
 [ 82.905,  96.701],
 [ 86.816,  97.549],
 [ 89.944,  98.303],
 [ 92.291,  98.775],
 [ 94.078,  99.246],
 [ 95.531,  99.434],
 [ 96.425,  99.717],
 [ 96.983,  99.906],
 [ 97.542, 100.   ],
 [ 97.654, 100.   ],
 [ 97.654, 100.   ],
 [ 97.654, 100.   ],
 [ 97.654,  99.811],
 [ 97.654,  99.623],
 [ 97.765,  99.057],
 [ 97.765,  98.586],
 [ 97.989,  97.738],
 [ 98.212,  96.513],
 [ 98.324,  94.91 ],
 [ 98.436,  93.402],
 [ 98.547,  91.423],
 [ 98.659,  89.538],
 [ 98.771,  87.276],
 [ 99.106,  85.391],
 [ 98.771,  83.318],
 [ 99.218,  82.187],
 [ 99.218,  81.056],
 [ 99.218,  80.113],
 [ 99.106,  79.453],
 [ 98.771,  79.076]]


zero_x_test = [
 [ 52.252,   0.   ],
 [ 47.748,   5.179],
 [ 42.703,  10.558],
 [ 42.703,  15.936],
 [ 38.198,  26.295],
 [ 33.153,  42.032],
 [ 33.153,  63.147],
 [ 33.153,  73.705],
 [ 38.198,  89.641],
 [ 42.703,  94.821],
 [ 47.748, 100.   ],
 [ 57.297, 100.   ],
 [ 66.847,  94.821],
 [ 76.216,  94.821],
 [ 85.586,  84.263],
 [ 90.45 ,  73.705],
 [ 95.135,  63.147],
 [100.   ,  47.41 ],
 [100.   ,  36.853],
 [ 95.135,  26.295],
 [ 90.45 ,  15.936],
 [ 80.901,  10.558],
 [ 66.847,  10.558],
 [ 52.252,  10.558],
 [ 38.198,  15.936],
 [ 28.649,  21.116],
 [ 14.234,  31.474],
 [  4.685,  36.853],
 [  0.   ,  42.032]]

x_test = [[  0.,     29.924],
 [  7.558,  20.076],
 [  7.558,  20.076],
 [  7.558,  29.924],
 [ 15.407,  39.773],
 [ 15.407,  59.848],
 [ 15.407,  70.076],
 [ 15.407,  80.303],
 [ 22.965,  90.152],
 [ 30.814, 100.   ],
 [ 38.372, 100.   ],
 [ 46.221, 100.   ],
 [ 61.628,  90.152],
 [ 77.035,  80.303],
 [ 84.593,  59.848],
 [ 92.442,  50.   ],
 [100.   ,  29.924],
 [100.   ,  20.076],
 [ 92.442,   9.848],
 [ 92.442,   0.   ],
 [ 77.035,   0.   ],
 [ 69.186,   9.848],
 [ 61.628,  20.076],
 [ 46.221,  29.924],
 [ 38.372,  39.773],
 [ 38.372,  50.   ],
 [ 38.372,  50.   ]]


x_test = np.array(x_test)
#x_test = np.flip(x_test, 1) #reverse of the sum sign
x_test = x_test.reshape(1, 27 ,2)

pre_out = model.predict(x_test, batch_size=None, verbose=1)
print(pre_out)

print("max index is")
print( np.argmax(pre_out, axis = 1))

# cvscores.append(scores[1] * 100)
# print("%.2f%% (+/- %.2f%%)" % (numpy.mean(cvscores), numpy.std(cvscores)))

#post analysis of model fit and evaluate
#https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/


#model.add((Dense(3, activation='sigmoid')))   results:

# 5/5 [==============================]5/5 [==============================] - 0s 27ms/step

# [0.3601095378398895, 0.07460707426071167, 0.800000011920929]
# training data results: 
# loss: 36.01095378398895
# mean_squared_error: 7.460707426071167
# acc: 80.0000011920929

# 5/5 [==============================]5/5 [==============================] - 0s 26ms/step

# [[0.08156254 0.03793901 0.88387424]
#  [0.21539919 0.01396057 0.47214362]
#  [0.73329604 0.02561312 0.06570691]
#  [0.14445683 0.75667053 0.02815693]
#  [0.07201358 0.6943321  0.05677234]]
# [Finished in 4.6s]

# model.add((Dense(3, activation='softmax')))  

# 5/5 [==============================]5/5 [==============================] - 0s 26ms/step

# [0.2547874450683594, 0.04909709841012955, 0.800000011920929]
# training data results: 
# loss: 25.478744506835938
# mean_squared_error: 4.909709841012955
# acc: 80.0000011920929

# 5/5 [==============================]5/5 [==============================] - 0s 27ms/step

# [[6.6355389e-04 2.3135196e-03 9.9702293e-01]
#  [3.0156642e-01 3.2326201e-01 3.7517160e-01]
#  [9.5761818e-01 2.7063403e-02 1.5318362e-02]
#  [7.4732224e-03 9.8555619e-01 6.9705443e-03]
#  [7.2073410e-03 9.8576677e-01 7.0258216e-03]]
# [Finished in 4.6s]
